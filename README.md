# üöÄ Sistema RAG - Node.js + Hono.js

Sistema completo de **RAG (Retrieval-Augmented Generation)** implementado em Node.js usando Hono.js. Processa documentos, gera embeddings, indexa em vector database e responde perguntas usando LLM local (Ollama).

## üìã O que o projeto faz?

Este sistema permite:

- ‚úÖ **Processar documentos** (PDF, DOCX, HTML, TXT) com suporte a OCR para PDFs escaneados
- ‚úÖ **Extrair e dividir texto** em chunks inteligentes com overlap
- ‚úÖ **Gerar embeddings** usando modelos locais (@xenova/transformers)
- ‚úÖ **Indexar documentos** em vector database customizada (JSON-based)
- ‚úÖ **Buscar documentos relevantes** usando similarity search
- ‚úÖ **Gerar respostas** usando LLM local (Ollama)
- ‚úÖ **API RESTful** para integra√ß√£o
- ‚úÖ **Deploy no Azure Functions** (serverless)

## üéØ Tecnologias Utilizadas

- **Hono.js**: Framework web r√°pido e leve
- **@xenova/transformers**: Embeddings locais (sem necessidade de API externa)
- **Ollama**: LLM local (gratuito)
- **Tesseract.js**: OCR para PDFs escaneados
- **TypeScript**: Tipagem estrita
- **Husky**: Git hooks para valida√ß√£o autom√°tica

## üì¶ Como Baixar e Instalar

### 1. Pr√©-requisitos

- **Node.js** 20+ instalado
- **Ollama** instalado e rodando
- **Git** (opcional, para versionamento)

### 2. Clonar/Baixar o projeto

```bash
# Se usar Git
git clone <seu-repositorio>
cd IA

# Ou baixe e extraia o projeto
```

### 3. Instalar depend√™ncias

```bash
npm install
```

### 4. Instalar e configurar Ollama

**macOS:**

```bash
brew install ollama
```

**Linux:**

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
Baixe de: https://ollama.ai/download

### 5. Baixar modelo LLM

```bash
ollama pull llama3.2
```

### 6. Configurar vari√°veis de ambiente (opcional)

Crie um arquivo `.env` na raiz:

```env
PORT=3000
OLLAMA_URL=http://localhost:11434
```

## üöÄ Como Rodar

### Desenvolvimento (com auto-reload):

```bash
npm run dev
```

### Produ√ß√£o:

```bash
npm start
```

### Ou compilar e rodar:

```bash
npm run build
node dist/server.js
```

O servidor estar√° dispon√≠vel em: `http://localhost:3000`

## üì° API Endpoints

### `GET /api/health`

Health check do sistema.

**Resposta:**

```json
{
  "status": "ok",
  "message": "RAG System running"
}
```

### `POST /api/query`

Upload de documento + query em uma √∫nica chamada.

**Form Data (multipart/form-data):**

- `file`: Arquivo (PDF, DOCX, TXT, HTML) - opcional
- `query`: Pergunta/consulta - obrigat√≥rio

**JSON (application/json):**

```json
{
  "query": "Qual √© o conte√∫do do documento?"
}
```

**Resposta:**

```json
{
  "success": true,
  "response": "Resposta gerada pelo LLM...",
  "sources": ["documento.pdf"],
  "metadata": {
    "model": "llama3.2",
    "numSources": 1
  },
  "fileProcessed": "documento.pdf"
}
```

### `POST /api/documents/upload`

Upload e processa documento (sem query).

**Form Data:**

- `file`: Arquivo (PDF, DOCX, TXT, HTML)

**Resposta:**

```json
{
  "success": true,
  "filename": "documento.pdf",
  "chunksCreated": 10,
  "metadata": { ... }
}
```

### `GET /api/collection/info`

Informa√ß√µes sobre a cole√ß√£o de documentos indexados.

**Resposta:**

```json
{
  "totalDocuments": 10,
  "totalChunks": 150
}
```

### `DELETE /api/collection`

Limpa toda a cole√ß√£o de documentos.

**Resposta:**

```json
{
  "success": true,
  "message": "Cole√ß√£o limpa"
}
```

## üìÅ Estrutura do Projeto

```
IA/
‚îú‚îÄ‚îÄ src/                    # C√≥digo fonte TypeScript
‚îÇ   ‚îú‚îÄ‚îÄ app.ts             # Aplica√ß√£o Hono centralizada (rotas e l√≥gica)
‚îÇ   ‚îú‚îÄ‚îÄ chunker.ts         # Divis√£o de texto em chunks
‚îÇ   ‚îú‚îÄ‚îÄ documentProcessor.ts  # Processamento de documentos (PDF, DOCX, etc)
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.ts      # Gera√ß√£o de embeddings
‚îÇ   ‚îú‚îÄ‚îÄ generator.ts        # Gera√ß√£o de respostas (LLM)
‚îÇ   ‚îú‚îÄ‚îÄ retriever.ts        # Busca de documentos relevantes
‚îÇ   ‚îú‚îÄ‚îÄ types.ts           # Tipos TypeScript compartilhados
‚îÇ   ‚îú‚îÄ‚îÄ utils.ts           # Fun√ß√µes utilit√°rias
‚îÇ   ‚îî‚îÄ‚îÄ vectorDb.ts        # Vector database customizada
‚îÇ
‚îú‚îÄ‚îÄ azure/                  # Configura√ß√£o para Azure Functions
‚îÇ   ‚îú‚îÄ‚îÄ index.ts           # Entry point para Azure Functions
‚îÇ   ‚îú‚îÄ‚îÄ function.json      # Configura√ß√£o da Function
‚îÇ   ‚îú‚îÄ‚îÄ host.json          # Configura√ß√£o do host
‚îÇ   ‚îú‚îÄ‚îÄ package.json       # Depend√™ncias espec√≠ficas do Azure
‚îÇ   ‚îî‚îÄ‚îÄ tsconfig.json      # Configura√ß√£o TypeScript para Azure
‚îÇ
‚îú‚îÄ‚îÄ dist/                   # Arquivos compilados (gerado automaticamente)
‚îú‚îÄ‚îÄ vector_db/              # Vector database (JSON files)
‚îÇ   ‚îî‚îÄ‚îÄ documents.json     # Documentos indexados
‚îÇ
‚îú‚îÄ‚îÄ server.ts               # Servidor Node.js (importa src/app.ts)
‚îú‚îÄ‚îÄ package.json            # Depend√™ncias e scripts
‚îú‚îÄ‚îÄ tsconfig.json           # Configura√ß√£o TypeScript
‚îú‚îÄ‚îÄ eslint.config.js        # Configura√ß√£o ESLint
‚îú‚îÄ‚îÄ .prettierrc.json        # Configura√ß√£o Prettier
‚îú‚îÄ‚îÄ .commitlintrc.json      # Configura√ß√£o Conventional Commits
‚îú‚îÄ‚îÄ .husky/                 # Git hooks (Husky)
‚îÇ   ‚îú‚îÄ‚îÄ pre-commit         # Valida lint e build antes do commit
‚îÇ   ‚îî‚îÄ‚îÄ commit-msg         # Valida formato do commit
‚îî‚îÄ‚îÄ README.md              # Este arquivo
```

## üìÑ Descri√ß√£o dos Arquivos

### C√≥digo Fonte (`src/`)

#### `src/app.ts`

**Aplica√ß√£o Hono centralizada** - Cont√©m todas as rotas e l√≥gica do sistema RAG. Este arquivo √© importado tanto pelo servidor Node.js (`server.ts`) quanto pelo Azure Functions (`azure/index.ts`), garantindo que a mesma l√≥gica funcione em ambos os ambientes.

**Rotas:**

- `GET /api/health` - Health check
- `POST /api/query` - Upload + query
- `POST /api/documents/upload` - Upload de documento
- `GET /api/collection/info` - Informa√ß√µes da cole√ß√£o
- `DELETE /api/collection` - Limpar cole√ß√£o

#### `src/chunker.ts`

**Divis√£o de texto em chunks** - Divide textos longos em peda√ßos menores com overlap configur√°vel. Garante que o contexto seja preservado entre chunks.

**Configura√ß√£o padr√£o:**

- `chunkSize`: 1000 tokens
- `chunkOverlap`: 200 tokens

#### `src/documentProcessor.ts`

**Processamento de documentos** - Suporta m√∫ltiplos formatos:

- **PDF**: Extra√ß√£o de texto + OCR autom√°tico para PDFs escaneados
- **DOCX**: Extra√ß√£o de texto usando mammoth
- **HTML/TXT**: Leitura direta

**Recursos:**

- OCR autom√°tico quando PDF tem pouco texto
- Suporte a arquivos at√© 200MB
- Extra√ß√£o de metadados

#### `src/embeddings.ts`

**Gera√ß√£o de embeddings** - Usa `@xenova/transformers` para gerar embeddings localmente.

**Modelo padr√£o:** `Xenova/all-MiniLM-L6-v2`

#### `src/vectorDb.ts`

**Vector database customizada** - Armazena documentos e embeddings em JSON. Implementa busca por similaridade usando cosine similarity.

**Recursos:**

- Armazenamento local (sem depend√™ncias externas)
- Busca por similaridade
- Filtros por metadados
- Limpeza autom√°tica por requisi√ß√£o

#### `src/retriever.ts`

**Sistema de recupera√ß√£o** - Busca documentos relevantes baseado na query do usu√°rio.

**Processo:**

1. Gera embedding da query
2. Busca documentos similares na vector DB
3. Retorna top K documentos mais relevantes

#### `src/generator.ts`

**Gera√ß√£o de respostas** - Usa Ollama (LLM local) para gerar respostas baseadas no contexto recuperado.

**Modelo padr√£o:** `llama3.2`

#### `src/types.ts`

**Tipos TypeScript compartilhados** - Define todas as interfaces e tipos usados no sistema.

#### `src/utils.ts`

**Fun√ß√µes utilit√°rias** - Fun√ß√µes auxiliares como `cosineSimilarity`.

### Arquivos de Configura√ß√£o

#### `server.ts`

**Servidor Node.js** - Entry point para desenvolvimento local. Importa `src/app.ts` e inicia o servidor HTTP na porta 3000.

#### `azure/index.ts`

**Entry point Azure Functions** - Adapta a aplica√ß√£o Hono para Azure Functions usando `@marplex/hono-azurefunc-adapter`.

#### `package.json`

**Depend√™ncias e scripts** - Gerencia todas as depend√™ncias do projeto e scripts npm.

**Scripts principais:**

- `npm start` - Inicia servidor
- `npm run dev` - Modo desenvolvimento (auto-reload)
- `npm run build` - Compila TypeScript
- `npm run lint` - Valida lint
- `npm run lint:fix` - Corrige lint automaticamente

#### `tsconfig.json`

**Configura√ß√£o TypeScript** - Define op√ß√µes de compila√ß√£o estritas:

- `strict: true`
- `noImplicitAny: true`
- `strictNullChecks: true`
- E mais...

#### `eslint.config.js`

**Configura√ß√£o ESLint** - Regras de linting para TypeScript:

- Pro√≠be uso de `any`
- Valida tipos
- Integrado com Prettier

#### `.prettierrc.json`

**Configura√ß√£o Prettier** - Formata√ß√£o autom√°tica de c√≥digo.

#### `.commitlintrc.json`

**Configura√ß√£o Conventional Commits** - Valida formato das mensagens de commit.

### Azure Functions (`azure/`)

#### `azure/index.ts`

**Entry point Azure Functions** - Importa `src/app.ts` e adapta para Azure Functions v4.

#### `azure/function.json`

**Configura√ß√£o da Function** - Define HTTP trigger com todos os m√©todos.

#### `azure/host.json`

**Configura√ß√£o do host** - Timeout, logging, etc.

#### `azure/package.json`

**Depend√™ncias do Azure** - Inclui `@azure/functions` e `@marplex/hono-azurefunc-adapter`.

## üêï Husky - Git Hooks

O projeto usa **Husky** para validar c√≥digo e commits automaticamente.

### O que √© Husky?

Husky √© uma ferramenta que executa scripts automaticamente em eventos do Git (como antes de fazer commit).

### Hooks Configurados

#### `.husky/pre-commit`

Executa **antes** de cada commit:

1. **lint-staged**: Valida e corrige lint apenas nos arquivos que ser√£o commitados
2. **Build**: Valida se o TypeScript compila sem erros

**Se algum teste falhar, o commit √© bloqueado!**

#### `.husky/commit-msg`

Valida o formato da mensagem de commit:

- ‚úÖ Deve seguir **Conventional Commits**
- ‚úÖ Formato: `tipo: descri√ß√£o`
- ‚úÖ Tipos permitidos: `feat`, `fix`, `docs`, `style`, `refactor`, `perf`, `test`, `build`, `ci`, `chore`, `revert`

### Como Fazer Commits

#### Op√ß√£o 1: Interface Interativa com Sugest√µes (Recomendado) üéØ

Use o **Commitizen** que oferece uma interface interativa passo a passo **com sugest√µes autom√°ticas** baseadas nos arquivos modificados:

```bash
npm run commit
# ou
git commit
```

**Antes de abrir o menu**, o sistema analisa seus arquivos modificados e mostra uma sugest√£o:

```
üìù Sugest√£o de Commit

Arquivos analisados:
  + 2 adicionado(s)
  ~ 5 modificado(s)

Sugest√£o:
  feat(api): 5 arquivo(s) modificado(s) indicam "feat"
  ‚úì Alta confian√ßa

Alternativas:
  ‚Ä¢ fix
  ‚Ä¢ refactor

Escopo sugerido: api
```

Depois voc√™ ver√° o menu interativo do Commitizen:

```
? Select the type of change that you're committing: (Use arrow keys)
‚ùØ feat:     A new feature
  fix:      A bug fix
  docs:     Documentation only changes
  style:    Changes that do not affect the meaning of the code
  refactor: A code change that neither fixes a bug nor adds a feature
  perf:     A code change that improves performance
  test:     Adding missing tests or correcting existing tests
  build:    Changes that affect the build system or external dependencies
  ci:       Changes to our CI configuration files and scripts
  chore:    Other changes that don't modify src or test files
  revert:   Reverts a previous commit
```

Depois voc√™ ser√° perguntado:

- **Scope** (opcional): Qual parte do c√≥digo foi afetada
- **Subject**: Descri√ß√£o curta do que foi feito
- **Body** (opcional): Descri√ß√£o detalhada
- **Breaking changes** (opcional): Se h√° mudan√ßas que quebram compatibilidade
- **Issues** (opcional): N√∫meros de issues relacionadas

**Exemplo de uso:**

```bash
$ npm run commit

? Select the type of change: feat
? What is the scope of this change: api
? Write a short, imperative tense description: adiciona endpoint de health check
? Provide a longer description: Adiciona endpoint GET /api/health para verificar status do sistema
? Are there any breaking changes? No
? Does this change affect any open issues? No

[master abc1234] feat(api): adiciona endpoint de health check
```

#### Op√ß√£o 2: Commit Manual

Se preferir escrever manualmente, use o formato:

```bash
git commit -m "tipo(escopo): descri√ß√£o"
```

**‚úÖ Exemplos v√°lidos:**

```bash
git commit -m "feat: adiciona suporte a Azure Functions"
git commit -m "fix: corrige erro de parsing de PDF"
git commit -m "docs: atualiza README"
git commit -m "feat(api): adiciona endpoint de health check"
git commit -m "fix(vectorDb): corrige busca por similaridade"
```

**‚ùå Exemplos inv√°lidos (ser√£o bloqueados):**

```bash
git commit -m "adiciona funcionalidade"        # Sem tipo
git commit -m "FEAT: adiciona"                # Tipo em mai√∫scula
git commit -m "feat: Adiciona funcionalidade" # Subject em mai√∫scula
git commit -m "feat:"                         # Sem subject
```

### Benef√≠cios

- ‚úÖ **Qualidade**: C√≥digo sempre validado antes do commit
- ‚úÖ **Consist√™ncia**: Formata√ß√£o autom√°tica
- ‚úÖ **Hist√≥rico**: Commits padronizados e f√°ceis de entender
- ‚úÖ **Preven√ß√£o**: Erros detectados antes de chegar ao reposit√≥rio

### Scripts Relacionados

```bash
# Fazer commit com interface interativa (recomendado)
npm run commit

# Ver sugest√£o de commit baseado nos arquivos modificados
npm run suggest

# Validar lint manualmente
npm run lint

# Corrigir problemas de lint automaticamente
npm run lint:fix

# Formatar c√≥digo
npm run format
```

### Como Funciona a Sugest√£o Autom√°tica?

O sistema analisa automaticamente:

- ‚úÖ **Arquivos adicionados/modificados/removidos**
- ‚úÖ **Tipo de mudan√ßa** (novo c√≥digo, corre√ß√£o, refatora√ß√£o)
- ‚úÖ **Localiza√ß√£o dos arquivos** (src/, test/, docs/, etc.)
- ‚úÖ **Conte√∫do das mudan√ßas** (novas fun√ß√µes, corre√ß√µes de bug, etc.)
- ‚úÖ **Escopo sugerido** baseado na estrutura de pastas

**Exemplos de detec√ß√£o:**

- Arquivos em `src/` com novas fun√ß√µes ‚Üí `feat`
- Arquivos de teste ‚Üí `test`
- Corre√ß√µes de erro ‚Üí `fix`
- Arquivos de documenta√ß√£o ‚Üí `docs`
- Mudan√ßas em `package.json` ‚Üí `build`
- Refatora√ß√£o de c√≥digo ‚Üí `refactor`

### Dica: Alias Git (Opcional)

Para usar `git commit` diretamente com interface interativa, adicione um alias:

```bash
git config --global alias.cz "!npm run commit"
```

Depois voc√™ pode usar:

```bash
git cz  # Abre a interface interativa
```

## üöÄ Deploy em Produ√ß√£o

Existem v√°rias op√ß√µes para rodar o sistema em produ√ß√£o. Escolha a que melhor se adequa ao seu caso:

### Op√ß√£o 1: Servidor Pr√≥prio (VPS/Cloud)

#### O que voc√™ precisa instalar na m√°quina:

1. **Node.js 20+**

   ```bash
   # Ubuntu/Debian
   curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
   sudo apt-get install -y nodejs

   # macOS (j√° deve ter se instalou via Homebrew)
   brew install node@20
   ```

2. **Ollama**

   ```bash
   # Linux
   curl -fsSL https://ollama.ai/install.sh | sh

   # macOS
   brew install ollama
   ```

3. **Depend√™ncias do sistema (para OCR e PDF)**

   ```bash
   # Ubuntu/Debian
   sudo apt-get update
   sudo apt-get install -y \
     build-essential \
     python3 \
     cairo-dev \
     jpeg-dev \
     pango-dev \
     graphicsmagick \
     imagemagick

   # macOS
   brew install graphicsmagick imagemagick
   ```

#### Passos para Deploy:

1. **Clonar/Baixar o projeto na m√°quina:**

   ```bash
   git clone <seu-repositorio>
   cd IA
   ```

2. **Instalar depend√™ncias:**

   ```bash
   npm install
   ```

3. **Baixar modelo LLM:**

   ```bash
   ollama pull llama3.2
   ```

4. **Configurar vari√°veis de ambiente:**

   ```bash
   # Criar arquivo .env
   cat > .env << EOF
   NODE_ENV=production
   PORT=3000
   OLLAMA_URL=http://localhost:11434
   EOF
   ```

5. **Compilar o projeto:**

   ```bash
   npm run build
   ```

6. **Iniciar Ollama (se n√£o estiver rodando):**

   ```bash
   ollama serve
   # Ou como servi√ßo systemd (veja abaixo)
   ```

7. **Iniciar a aplica√ß√£o:**

   ```bash
   # Op√ß√£o 1: Direto (n√£o recomendado para produ√ß√£o)
   npm start

   # Op√ß√£o 2: Com PM2 (recomendado)
   npm install -g pm2
   pm2 start dist/server.js --name rag-system
   pm2 save
   pm2 startup  # Configurar para iniciar no boot
   ```

#### Configurar Ollama como Servi√ßo (Linux):

Crie `/etc/systemd/system/ollama.service`:

```ini
[Unit]
Description=Ollama Service
After=network.target

[Service]
Type=simple
User=seu-usuario
ExecStart=/usr/local/bin/ollama serve
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
```

Ativar:

```bash
sudo systemctl enable ollama
sudo systemctl start ollama
```

#### Configurar Aplica√ß√£o como Servi√ßo (Linux):

Crie `/etc/systemd/system/rag-system.service`:

```ini
[Unit]
Description=RAG System API
After=network.target ollama.service
Requires=ollama.service

[Service]
Type=simple
User=seu-usuario
WorkingDirectory=/caminho/para/IA
Environment="NODE_ENV=production"
Environment="PORT=3000"
Environment="OLLAMA_URL=http://localhost:11434"
ExecStart=/usr/bin/node /caminho/para/IA/dist/server.js
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
```

Ativar:

```bash
sudo systemctl enable rag-system
sudo systemctl start rag-system
sudo systemctl status rag-system  # Verificar status
```

#### Configurar Nginx como Reverse Proxy (Opcional):

Crie `/etc/nginx/sites-available/rag-system`:

```nginx
server {
    listen 80;
    server_name seu-dominio.com;

    location / {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}
```

Ativar:

```bash
sudo ln -s /etc/nginx/sites-available/rag-system /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl reload nginx
```

### Op√ß√£o 2: Docker (Recomendado)

#### Pr√©-requisitos:

- **Docker** instalado
- **Docker Compose** instalado

#### Passos:

1. **Clonar o projeto:**

   ```bash
   git clone <seu-repositorio>
   cd IA
   ```

2. **Baixar modelo LLM (antes de iniciar):**

   ```bash
   # Iniciar apenas o Ollama primeiro
   docker-compose up -d ollama

   # Aguardar Ollama iniciar (30 segundos)
   sleep 30

   # Baixar modelo
   docker exec -it ia-ollama-1 ollama pull llama3.2
   ```

3. **Iniciar todos os servi√ßos:**

   ```bash
   docker-compose up -d
   ```

4. **Verificar logs:**

   ```bash
   docker-compose logs -f
   ```

5. **Parar servi√ßos:**
   ```bash
   docker-compose down
   ```

#### Vantagens do Docker:

- ‚úÖ Isolamento de depend√™ncias
- ‚úÖ F√°cil de atualizar
- ‚úÖ Port√°vel entre ambientes
- ‚úÖ Gerenciamento autom√°tico de servi√ßos
- ‚úÖ Volumes persistentes para dados

#### Configurar Docker para iniciar no boot:

```bash
# Criar arquivo docker-compose.service
sudo nano /etc/systemd/system/docker-compose-rag.service
```

Conte√∫do:

```ini
[Unit]
Description=RAG System Docker Compose
Requires=docker.service
After=docker.service

[Service]
Type=oneshot
RemainAfterExit=yes
WorkingDirectory=/caminho/para/IA
ExecStart=/usr/local/bin/docker-compose up -d
ExecStop=/usr/local/bin/docker-compose down
TimeoutStartSec=0

[Install]
WantedBy=multi-user.target
```

Ativar:

```bash
sudo systemctl enable docker-compose-rag
sudo systemctl start docker-compose-rag
```

### Op√ß√£o 3: Azure Functions (Serverless)

#### Pr√©-requisitos:

1. **Azure CLI** instalado
2. **Azure Functions Core Tools** instalado
3. Conta Azure com subscription ativa

#### Limita√ß√µes:

‚ö†Ô∏è **IMPORTANTE**: Azure Functions tem limita√ß√µes:

- Timeout m√°ximo: 10 minutos (configurado em `azure/host.json`)
- Mem√≥ria limitada
- Ollama precisa estar em outro servi√ßo (Azure VM, Container Instance, etc.)
- N√£o recomendado para arquivos muito grandes (>50MB)

#### Passos:

1. **Criar Function App no Azure:**

   ```bash
   az group create --name rag-resource-group --location eastus
   az storage account create --name ragstorage --location eastus --resource-group rag-resource-group --sku Standard_LRS
   az functionapp create --resource-group rag-resource-group --consumption-plan-location eastus --runtime node --runtime-version 20 --functions-version 4 --name rag-system-api --storage-account ragstorage
   ```

2. **Compilar projeto:**

   ```bash
   npm run build
   ```

3. **Copiar arquivos para azure:**

   ```bash
   npm run copy:azure
   ```

4. **Instalar depend√™ncias do Azure:**

   ```bash
   cd azure
   npm install
   ```

5. **Compilar Azure:**

   ```bash
   npm run build
   ```

6. **Configurar vari√°veis de ambiente no Azure:**

   ```bash
   az functionapp config appsettings set --name rag-system-api --resource-group rag-resource-group --settings OLLAMA_URL=http://seu-ollama-url:11434
   ```

7. **Deploy:**
   ```bash
   func azure functionapp publish rag-system-api
   ```

#### Configurar Ollama separadamente (Azure):

Como Azure Functions n√£o pode rodar Ollama diretamente, voc√™ precisa:

**Op√ß√£o A: Azure Container Instance**

```bash
az container create \
  --resource-group rag-resource-group \
  --name ollama-container \
  --image ollama/ollama:latest \
  --dns-name-label ollama-rag \
  --ports 11434 \
  --cpu 4 \
  --memory 8
```

**Op√ß√£o B: Azure VM**

- Criar VM Linux
- Instalar Ollama
- Configurar firewall para permitir acesso

### Op√ß√£o 4: Outros Cloud Providers

#### AWS (EC2 + ECS ou Lambda)

- Similar ao Azure Functions
- Ollama em EC2 ou ECS
- API em Lambda ou ECS

#### Google Cloud (Cloud Run + Compute Engine)

- Ollama em Compute Engine
- API em Cloud Run

#### DigitalOcean (Droplet)

- Similar ao servidor pr√≥prio
- Droplet com Node.js + Ollama

## üìä Compara√ß√£o das Op√ß√µes

| Op√ß√£o                | Complexidade | Custo    | Escalabilidade | Recomendado Para                      |
| -------------------- | ------------ | -------- | -------------- | ------------------------------------- |
| **Servidor Pr√≥prio** | M√©dia        | Baixo    | M√©dia          | Projetos pequenos/m√©dios              |
| **Docker**           | Baixa        | Baixo    | M√©dia          | ‚úÖ **Recomendado** - F√°cil manuten√ß√£o |
| **Azure Functions**  | Alta         | M√©dio    | Alta           | Serverless, alto tr√°fego              |
| **Outros Cloud**     | Alta         | Vari√°vel | Alta           | Empresas grandes                      |

## üîß Configura√ß√µes de Produ√ß√£o

### Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz:

```env
# Ambiente
NODE_ENV=production

# Porta do servidor
PORT=3000

# URL do Ollama (ajuste conforme sua instala√ß√£o)
OLLAMA_URL=http://localhost:11434
# Para Docker: http://ollama:11434
# Para servidor remoto: http://ip-do-servidor:11434

# Opcional: Configura√ß√µes de timeout
REQUEST_TIMEOUT=300000
```

### Otimiza√ß√µes de Produ√ß√£o

1. **Aumentar mem√≥ria do Node.js:**

   ```bash
   NODE_OPTIONS="--max-old-space-size=4096" npm start
   ```

2. **Usar PM2 para gerenciamento:**

   ```bash
   npm install -g pm2
   pm2 start dist/server.js --name rag-api --max-memory-restart 2G
   ```

3. **Configurar logs:**

   ```bash
   # PM2 salva logs automaticamente
   pm2 logs rag-api
   ```

4. **Monitoramento:**
   ```bash
   # Health check endpoint
   curl http://localhost:3000/api/health
   ```

## üß™ Testar em Produ√ß√£o

### 1. Health Check:

```bash
curl http://seu-servidor:3000/api/health
```

### 2. Upload e Query:

```bash
curl -X POST http://seu-servidor:3000/api/query \
  -F "file=@documento.pdf" \
  -F "query=Qual √© o conte√∫do do documento?"
```

### 3. Verificar logs:

```bash
# Docker
docker-compose logs -f rag-api

# PM2
pm2 logs rag-api

# Systemd
journalctl -u rag-system -f
```

## üêõ Troubleshooting Produ√ß√£o

### Ollama n√£o conecta:

```bash
# Verificar se Ollama est√° rodando
curl http://localhost:11434/api/tags

# Verificar vari√°vel de ambiente
echo $OLLAMA_URL

# Testar conex√£o
curl http://ollama-url:11434/api/tags
```

### Erro de mem√≥ria:

```bash
# Aumentar mem√≥ria do Node.js
NODE_OPTIONS="--max-old-space-size=4096" npm start

# Ou no PM2
pm2 restart rag-api --update-env --max-memory-restart 4G
```

### Porta j√° em uso:

```bash
# Verificar o que est√° usando a porta
lsof -i :3000

# Mudar porta no .env
PORT=3001
```

### Docker n√£o inicia:

```bash
# Verificar logs
docker-compose logs

# Reconstruir imagens
docker-compose build --no-cache
docker-compose up -d
```

Veja a documenta√ß√£o completa em: [Guia de Deploy Azure](https://docs.microsoft.com/azure/azure-functions/)

## üß™ Testando a API

### Usando curl:

```bash
# Health check
curl http://localhost:3000/api/health

# Query com arquivo
curl -X POST http://localhost:3000/api/query \
  -F "file=@documento.pdf" \
  -F "query=Qual √© o conte√∫do do documento?"

# Query sem arquivo (usa documentos j√° indexados)
curl -X POST http://localhost:3000/api/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Qual √© o tema principal?"}'
```

### Usando Postman:

Importe a collection: `RAG_API.postman_collection.json`

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### Vari√°veis de Ambiente

Crie um arquivo `.env`:

```env
PORT=3000
OLLAMA_URL=http://localhost:11434
```

### Ajustar Tamanho de Chunks

Edite `src/app.ts`:

```typescript
const chunker = new TextChunker({
  chunkSize: 1000, // Tamanho do chunk
  chunkOverlap: 200, // Overlap entre chunks
});
```

### Mudar Modelo de Embeddings

Edite `src/app.ts`:

```typescript
const embeddingGenerator = new EmbeddingGenerator({
  model: "Xenova/all-MiniLM-L6-v2",
});
```

### Mudar Modelo LLM

Edite `src/app.ts` ou vari√°vel de ambiente:

```typescript
const responseGenerator = new ResponseGenerator({
  model: "llama3.2",
  ollamaUrl: process.env.OLLAMA_URL || "http://localhost:11434",
});
```

## üêõ Troubleshooting

### Erro: "Ollama n√£o encontrado"

```bash
# Verificar se Ollama est√° rodando
ollama list

# Iniciar Ollama (se necess√°rio)
ollama serve
```

### Erro: "Modelo n√£o carregado"

```bash
# Baixar modelo
ollama pull llama3.2
```

### Erro: "Cannot find module"

```bash
# Reinstalar depend√™ncias
npm install
```

### Commit bloqueado pelo Husky

```bash
# Corrigir lint automaticamente
npm run lint:fix

# Ou formatar c√≥digo
npm run format
```

### Erro de build

```bash
# Ver erros detalhados
npm run build

# Corrigir erros de TypeScript
```

## üìö Recursos Adicionais

- [Documenta√ß√£o Hono.js](https://hono.dev/docs/)
- [Ollama](https://ollama.ai/)
- [Conventional Commits](https://www.conventionalcommits.org/)
- [Azure Functions](https://docs.microsoft.com/azure/azure-functions/)

## üìù Licen√ßa

MIT

## ü§ù Contribuindo

1. Fa√ßa fork do projeto
2. Crie uma branch (`git checkout -b feat/nova-funcionalidade`)
3. Commit suas mudan√ßas (`git commit -m "feat: adiciona nova funcionalidade"`)
4. Push para a branch (`git push origin feat/nova-funcionalidade`)
5. Abra um Pull Request

**Nota:** Commits devem seguir Conventional Commits (validado automaticamente pelo Husky).
